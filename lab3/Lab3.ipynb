{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f637e67e",
   "metadata": {},
   "source": [
    "Lab3"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c655cf2b",
   "metadata": {},
   "source": [
    "Import the label from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66becd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "csv_file = 'Chinese_MINST_Dataset/chinese_mnist.csv'\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "\n",
    "data['label'] = data.apply(lambda x: f\"{x['suite_id']}_{x['sample_id']}_{x['code']}.jpg\", axis=1)\n",
    "data['value'] = data['value']  \n",
    "\n",
    "final_data = data[['label', 'value', 'code']]  \n",
    "\n",
    "train_samples = []\n",
    "test_samples = []"
   ]
  },
  {
   "cell_type": "raw",
   "id": "df827873",
   "metadata": {},
   "source": [
    "According to the filter specification, sample the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b711f8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_train_per_class = 333\n",
    "min_test_per_class = 66\n",
    "\n",
    "total_train_samples = 5000\n",
    "total_test_samples = 1000\n",
    "\n",
    "class_distribution = final_data['code'].value_counts()\n",
    "\n",
    "\n",
    "for code, group in final_data.groupby('code'):\n",
    "    group = group.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    available_samples = len(group)\n",
    "    train_count = min(min_train_per_class, available_samples - min_test_per_class) \n",
    "    test_count = min(min_test_per_class, available_samples - train_count)  \n",
    "    \n",
    "    train_samples.append(group.iloc[:train_count]) \n",
    "    test_samples.append(group.iloc[train_count:train_count + test_count]) \n",
    "\n",
    "train_data = pd.concat(train_samples).reset_index(drop=True)\n",
    "test_data = pd.concat(test_samples).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "032392c5",
   "metadata": {},
   "source": [
    "Get the 1000 test data and 5000 train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18614175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insufficient samples in train or test dataset, trying to supplement...\n",
      "Train set size: 5000\n",
      "Test set size: 1000\n"
     ]
    }
   ],
   "source": [
    "if len(train_data) < total_train_samples or len(test_data) < total_test_samples:\n",
    "    print(\"Insufficient samples in train or test dataset, trying to supplement...\")\n",
    "    \n",
    "  \n",
    "    remaining_data = final_data[~final_data['label'].isin(train_data['label']) & ~final_data['label'].isin(test_data['label'])]\n",
    "    \n",
    "\n",
    "    while len(train_data) < total_train_samples:\n",
    "        additional_samples = remaining_data.sample(n=1, random_state=42)\n",
    "        train_data = pd.concat([train_data, additional_samples]).reset_index(drop=True)\n",
    "        remaining_data = remaining_data[~remaining_data['label'].isin(additional_samples['label'])]\n",
    "        \n",
    "\n",
    "    while len(test_data) < total_test_samples:\n",
    "        additional_samples = remaining_data.sample(n=1, random_state=42)\n",
    "        test_data = pd.concat([test_data, additional_samples]).reset_index(drop=True)\n",
    "        remaining_data = remaining_data[~remaining_data['label'].isin(additional_samples['label'])]\n",
    "\n",
    "print(f'Train set size: {len(train_data)}')\n",
    "print(f'Test set size: {len(test_data)}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4f10686",
   "metadata": {},
   "source": [
    "Preapring the (pixels,value) data according to the pairs above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd2ba821",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kandi/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import cv2  \n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "image_directory = 'Chinese_MINST_Dataset/data/data/input_'  \n",
    "\n",
    "\n",
    "\n",
    "def load_images_and_labels(data):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        img_path = image_directory + row['label']  \n",
    "        value = row['value'] \n",
    "        \n",
    "        # Load image\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  \n",
    "        if img is not None:\n",
    "            img_resized = cv2.resize(img, (64, 64)) \n",
    "            images.append(img_resized.flatten())  \n",
    "            labels.append(value) \n",
    "        else:\n",
    "            print(f\"Warning: Image {img_path} not found.\")\n",
    "    \n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e46e8c40",
   "metadata": {},
   "source": [
    "Standardize the data, then train it and show the performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff6fd182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kandi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:574: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Performance:\n",
      "Accuracy: 0.3830\n",
      "Precision: 0.4981\n",
      "Recall: 0.3830\n",
      "F1 Score: 0.3809\n",
      "Confusion Matrix:\n",
      "[[49  5  0  2  3  1  0  0  0  0  1  0  2  3  0]\n",
      " [ 1 62  2  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 27 25 12  0  1  0  0  1  0  0  0  0  0  0]\n",
      " [ 0 17 15 32  0  2  0  0  0  0  0  0  0  0  0]\n",
      " [ 6 20  3  7 25  1  1  0  0  0  1  0  0  1  1]\n",
      " [ 1  8 28 17  1 11  0  0  2  0  2  1  0  0  0]\n",
      " [ 2 39  0  1  1  0 18  0  2  0  0  0  1  2  0]\n",
      " [ 0 21  6  5  0  4  4 19  0  1  5  1  1  0  1]\n",
      " [ 0 10  5  0  0  1  1  0 51  0  0  0  0  0  0]\n",
      " [ 2 20  1  1  1  4 11  6  2 11  0  1  0  4  2]\n",
      " [ 0 18  3  0  0  0  3  0  0  0 26  0 14  2  0]\n",
      " [ 3 11  5  7  5  5  1  2  2  0  1 12  2  6  4]\n",
      " [ 1 20  5  3  0  0  3  0  0  1 20  0 13  0  0]\n",
      " [ 2 25  0  0  1  2 13  0  0  1  2  1  2 16  1]\n",
      " [ 1 18  5 10  3  3  2  6  2  1  0  0  3  0 13]]\n",
      "\n",
      "\n",
      "Decision Tree Performance:\n",
      "Accuracy: 0.2810\n",
      "Precision: 0.2852\n",
      "Recall: 0.2810\n",
      "F1 Score: 0.2809\n",
      "Confusion Matrix:\n",
      "[[22  1  1  0  7  2 10  1  2  0  2  8  7  1  2]\n",
      " [ 0 45  6  2  3  1  1  2  3  1  0  1  0  0  1]\n",
      " [ 0 10 19  8  1  2  4  2  4  3  2  2  4  3  2]\n",
      " [ 1  0  4 20  4  2  7  7  2  6  0  5  3  4  1]\n",
      " [ 2  1  1  2 12  8  5  4  1  5  0  7  3  5 10]\n",
      " [ 4  2  5  2  2 15  7  8  4  4  1  7  3  3  4]\n",
      " [ 1  0  2  7  4  2 12  3  4  5  5  2  6  5  8]\n",
      " [ 1  1  5  4  5  2  3 18 10  4  2  3  4  1  5]\n",
      " [ 2  8  6  1  1  0  5  4 21  8  4  2  2  4  0]\n",
      " [ 1  1  1  2  1  6  4 11  4 10  2  4  2  4 13]\n",
      " [ 1  4  0  1  1  4  2  2  2  1 31  3  9  3  2]\n",
      " [ 2  0  3  5  5  8  4  4  2  0  2 14  2  7  8]\n",
      " [ 2  1  6  4  2  6  1  3  5  1  8  5 16  4  2]\n",
      " [ 1  0  2  8  5 10  3  6  4  2  4  4  6  9  2]\n",
      " [ 1  1  2  3  9  4  7  3  4  6  1  5  1  3 17]]\n",
      "\n",
      "\n",
      "SGD Classifier Performance:\n",
      "Accuracy: 0.4560\n",
      "Precision: 0.4765\n",
      "Recall: 0.4560\n",
      "F1 Score: 0.4596\n",
      "Confusion Matrix:\n",
      "[[46  0  1  1  2  0  0  1  0  0  0  6  3  6  0]\n",
      " [ 0 44  9  3  0  0  0  1  1  1  4  0  1  2  0]\n",
      " [ 1  9 28 12  0  3  2  2  1  1  1  1  1  0  4]\n",
      " [ 1  1  9 24  1  2  3  7  1  6  2  3  3  1  2]\n",
      " [ 2  0  2  0 30  2  2  5  0  0  0  5  1  7 10]\n",
      " [ 2  1  2  2  1 31  0  4  0  8  2  9  1  4  4]\n",
      " [ 2  0  3  5  2  2 25  8  1  3  2  1  5  5  2]\n",
      " [ 1  0  5  1  1  4  0 32  3  5  3  0  3  3  7]\n",
      " [ 0  2 10  0  0  2  7  6 33  1  0  1  1  2  3]\n",
      " [ 1  0  2  0  0  8  5  5  0 25  1  2  3  7  7]\n",
      " [ 2  1  0  0  2  0  4  4  1  0 26  1 13 10  2]\n",
      " [ 2  0  1  1  1  9  1  4  2  5  3 19  3 13  2]\n",
      " [ 4  0  1  1  1  2  3  2  0  2 15  1 23  7  4]\n",
      " [ 3  0  1  0  2  1  3  6  1  2  4  6  4 29  4]\n",
      " [ 2  0  0  0  1  1  3  6  2  4  0  0  5  2 41]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_images_and_labels(train_data)  \n",
    "X_test, y_test = load_images_and_labels(test_data)    \n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)  \n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) \n",
    "X_test = scaler.transform(X_test)         \n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)  # KNN\n",
    "dt = DecisionTreeClassifier()               # Decision Tree\n",
    "sgd = SGDClassifier(max_iter=250)           # SGD Classifier\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "dt.fit(X_train, y_train)\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "knn_pred = knn.predict(X_test)\n",
    "dt_pred = dt.predict(X_test)\n",
    "sgd_pred = sgd.predict(X_test)\n",
    "\n",
    "def report_metrics(y_true, y_pred, model_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    print(f\"{model_name} Performance:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Report metrics for each classifier\n",
    "report_metrics(y_test, knn_pred, \"KNN\")\n",
    "report_metrics(y_test, dt_pred, \"Decision Tree\")\n",
    "report_metrics(y_test, sgd_pred, \"SGD Classifier\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0b642a64",
   "metadata": {},
   "source": [
    "Repeat all the experiment in 10000 train cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15d24852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insufficient samples in train or test dataset, trying to supplement...\n",
      "Train set size: 10000\n",
      "Test set size: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kandi/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:574: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Performance:\n",
      "Accuracy: 0.4370\n",
      "Precision: 0.5425\n",
      "Recall: 0.4370\n",
      "F1 Score: 0.4451\n",
      "Confusion Matrix:\n",
      "[[44  4  0  2  1  1  0  1  0  0  3  2  1  7  0]\n",
      " [ 0 60  5  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 2 22 30  9  0  1  0  1  1  0  0  0  0  0  0]\n",
      " [ 1 19 13 30  0  4  0  0  0  0  0  1  1  0  0]\n",
      " [ 4 18  3  3 30  1  2  2  0  0  0  1  0  0  2]\n",
      " [ 1  9 22 15  0 14  0  1  1  0  2  0  0  1  0]\n",
      " [ 0 34  2  0  0  0 26  1  1  0  0  0  0  2  0]\n",
      " [ 0 23  5  3  1  0  3 22  0  1  4  0  1  0  3]\n",
      " [ 0  2  2  0  0  0  2  0 60  0  0  0  0  0  0]\n",
      " [ 2 14  0  1  1  5 11  9  1 16  0  0  0  3  3]\n",
      " [ 0 19  2  0  0  0  2  0  0  0 28  0 13  2  0]\n",
      " [ 1  8  5  4  5  4  3  4  0  1  2 18  2 10  3]\n",
      " [ 1 19  6  2  0  0  4  1  0  0 17  0 18  0  0]\n",
      " [ 1 30  0  1  1  2  8  0  0  1  3  2  1 16  0]\n",
      " [ 2 17  4  3  3  2  1  5  1  1  0  1  2  0 25]]\n",
      "\n",
      "\n",
      "Decision Tree Performance:\n",
      "Accuracy: 0.3050\n",
      "Precision: 0.3026\n",
      "Recall: 0.3050\n",
      "F1 Score: 0.3026\n",
      "Confusion Matrix:\n",
      "[[28  1  1  0  1  5  1  2  0  6  1  9  5  1  5]\n",
      " [ 0 52  6  1  0  1  0  2  2  1  0  1  0  0  0]\n",
      " [ 0  8 29  6  3  1  3  2  5  1  2  1  1  3  1]\n",
      " [ 1  2 13 19  2  6  2  5  2  1  2  5  5  3  1]\n",
      " [ 3  2  4  6 18  5  4  2  3  5  1  3  1  2  7]\n",
      " [ 3  0  4  6  2 15  1  6  1  5  0 10  5  5  3]\n",
      " [ 0  0  4  9  2  6 12  5  3  2  4  1  4  7  7]\n",
      " [ 2  2  3  3  3  3  5 11  8  8  3  2  2  9  2]\n",
      " [ 3  4  4  4  1  0  5  3 29  3  1  5  2  1  1]\n",
      " [ 2  0  3  1  5  2  6 12  6 10  1  3  0  6  9]\n",
      " [ 0  2  4  0  3  2  4  7  1  2 26  0 13  1  1]\n",
      " [ 5  0  0  5  7  4  1  7  1  3  4 13  9  7  4]\n",
      " [ 3  1  2  5  5  1  3  0  0  3 13  5 19  4  4]\n",
      " [ 3  0  6  3  3  7  5  2  5  4  1  8  6  9  4]\n",
      " [ 3  1  2  5  6  2  6 11  2  4  0  4  3  3 15]]\n",
      "\n",
      "\n",
      "SGD Classifier Performance:\n",
      "Accuracy: 0.4590\n",
      "Precision: 0.4879\n",
      "Recall: 0.4590\n",
      "F1 Score: 0.4657\n",
      "Confusion Matrix:\n",
      "[[47  0  0  2  1  0  0  2  0  0  0  7  3  3  1]\n",
      " [ 0 44  8  3  1  0  0  0  2  0  6  0  1  1  0]\n",
      " [ 1  5 27 12  0  1  2  5  1  0  3  0  3  3  3]\n",
      " [ 1  0  6 31  1  3  1  6  0  3  3  3  6  3  2]\n",
      " [ 1  0  1  0 26  1  1  6  0  0  1  4  3  9 13]\n",
      " [ 2  0  2  3  2 26  1 10  0  3  2  3  2  7  3]\n",
      " [ 2  1  2  3  1  0 26  8  0  7  3  1  6  5  1]\n",
      " [ 1  0  1  1  1  2  0 31  2  6  6  1  0  6  8]\n",
      " [ 0  2 11  0  0  1  8  2 36  1  1  1  1  2  0]\n",
      " [ 1  0  2  0  0  4  0  8  0 24  3  1  3 11  9]\n",
      " [ 1  1  1  0  2  0  2  3  3  1 32  2 11  6  1]\n",
      " [ 2  0  0  0  3  8  1  3  1  3  6 22  3 17  1]\n",
      " [ 2  0  0  1  3  1  4  1  1  2 16  0 27  6  4]\n",
      " [ 3  0  0  2  2  0  1  3  1  6  1  6  3 33  5]\n",
      " [ 3  0  1  0  2  1  7  5  1 11  0  3  4  2 27]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "csv_file = 'Chinese_MINST_Dataset/chinese_mnist.csv'\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "\n",
    "data['label'] = data.apply(lambda x: f\"{x['suite_id']}_{x['sample_id']}_{x['code']}.jpg\", axis=1)\n",
    "data['value'] = data['value']  \n",
    "\n",
    "final_data = data[['label', 'value', 'code']]  \n",
    "\n",
    "train_samples = []\n",
    "test_samples = []\n",
    "\n",
    "min_train_per_class = 333\n",
    "min_test_per_class = 66\n",
    "\n",
    "total_train_samples = 10000\n",
    "total_test_samples = 1000\n",
    "\n",
    "class_distribution = final_data['code'].value_counts()\n",
    "\n",
    "\n",
    "for code, group in final_data.groupby('code'):\n",
    "    group = group.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    available_samples = len(group)\n",
    "    train_count = min(min_train_per_class, available_samples - min_test_per_class) \n",
    "    test_count = min(min_test_per_class, available_samples - train_count)  \n",
    "    \n",
    "    train_samples.append(group.iloc[:train_count]) \n",
    "    test_samples.append(group.iloc[train_count:train_count + test_count]) \n",
    "\n",
    "train_data = pd.concat(train_samples).reset_index(drop=True)\n",
    "test_data = pd.concat(test_samples).reset_index(drop=True)\n",
    "\n",
    "if len(train_data) < total_train_samples or len(test_data) < total_test_samples:\n",
    "    print(\"Insufficient samples in train or test dataset, trying to supplement...\")\n",
    "    \n",
    "  \n",
    "    remaining_data = final_data[~final_data['label'].isin(train_data['label']) & ~final_data['label'].isin(test_data['label'])]\n",
    "    \n",
    "\n",
    "    while len(train_data) < total_train_samples:\n",
    "        additional_samples = remaining_data.sample(n=1, random_state=42)\n",
    "        train_data = pd.concat([train_data, additional_samples]).reset_index(drop=True)\n",
    "        remaining_data = remaining_data[~remaining_data['label'].isin(additional_samples['label'])]\n",
    "        \n",
    "\n",
    "    while len(test_data) < total_test_samples:\n",
    "        additional_samples = remaining_data.sample(n=1, random_state=42)\n",
    "        test_data = pd.concat([test_data, additional_samples]).reset_index(drop=True)\n",
    "        remaining_data = remaining_data[~remaining_data['label'].isin(additional_samples['label'])]\n",
    "\n",
    "print(f'Train set size: {len(train_data)}')\n",
    "print(f'Test set size: {len(test_data)}')\n",
    "\n",
    "import cv2  # OpenCV for image processing\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "image_directory = 'Chinese_MINST_Dataset/data/data/input_'  \n",
    "\n",
    "\n",
    "\n",
    "def load_images_and_labels(data):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        img_path = image_directory + row['label']  \n",
    "        value = row['value'] \n",
    "        \n",
    "        # Load image\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  \n",
    "        if img is not None:\n",
    "            img_resized = cv2.resize(img, (64, 64)) \n",
    "            images.append(img_resized.flatten())  \n",
    "            labels.append(value) \n",
    "        else:\n",
    "            print(f\"Warning: Image {img_path} not found.\")\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "\n",
    "X_train, y_train = load_images_and_labels(train_data)  \n",
    "X_test, y_test = load_images_and_labels(test_data)    \n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)  \n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) \n",
    "X_test = scaler.transform(X_test)         \n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)  # KNN\n",
    "dt = DecisionTreeClassifier()               # Decision Tree\n",
    "sgd = SGDClassifier(max_iter=250)           # SGD Classifier\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "dt.fit(X_train, y_train)\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "knn_pred = knn.predict(X_test)\n",
    "dt_pred = dt.predict(X_test)\n",
    "sgd_pred = sgd.predict(X_test)\n",
    "\n",
    "def report_metrics(y_true, y_pred, model_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    print(f\"{model_name} Performance:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Report metrics for each classifier\n",
    "report_metrics(y_test, knn_pred, \"KNN\")\n",
    "report_metrics(y_test, dt_pred, \"Decision Tree\")\n",
    "report_metrics(y_test, sgd_pred, \"SGD Classifier\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0985d51f",
   "metadata": {},
   "source": [
    "SGD performs better than the other two in both cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
